{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class State:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros([3, 3])\n",
    "        self.result = 0 # 1 for p1, 2 for p2, -1 for draw, 0 for ongoing game\n",
    "        self.turn = 1\n",
    "        \n",
    "    def legal_actions(self):\n",
    "        if self.result != 0:\n",
    "            return []\n",
    "        \n",
    "        board = self.board.reshape(-1)\n",
    "        indices = np.array(range(len(board)), dtype=int)\n",
    "        return indices[board == 0]\n",
    "    \n",
    "    def update(self, action):\n",
    "        # assumed action is legal\n",
    "        self.board[action//3, action%3] = self.turn\n",
    "        self.turn = 3 - self.turn # turn changes even if game has ended but doesnt matter\n",
    "\n",
    "        for p in range(1, 3):\n",
    "            for i in range(3):\n",
    "                if (self.board[i, :] == p).all():\n",
    "                    self.result = p\n",
    "            for j in range(3):\n",
    "                if (self.board[:, j] == p).all():\n",
    "                    self.result = p\n",
    "            \n",
    "            if self.board[0, 0] == self.board[1, 1] == self.board[2, 2] == p:\n",
    "                self.result = p\n",
    "            if self.board[0, 2] == self.board[1, 1] == self.board[2, 0] == p:\n",
    "                self.result = p\n",
    "            \n",
    "        if self.result == 0 and (self.board.reshape(-1) != 0).all():\n",
    "            self.result = -1\n",
    "        \n",
    "        return self.get_index()\n",
    "    \n",
    "    def get_index(self):\n",
    "        res = np.full(27, 0)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                for k in range(3):\n",
    "                    res[i*9+j*3+k] = (self.board[i, j] == k).astype(float)\n",
    "#         print(res)\n",
    "        return torch.tensor(res, dtype=torch.float32).view([1, -1])\n",
    "#         return torch.tensor(\n",
    "#             [-1 if i == 2 else i for i in self.board.reshape(-1)],\n",
    "#             dtype=torch.float32).view(1, -1)\n",
    "#         board = self.board.reshape(-1)\n",
    "#         res = 0\n",
    "#         for i in range(len(board)):\n",
    "#             res = res*3 + board[i]\n",
    "#         return res\n",
    "    \n",
    "    def get_reward(self):\n",
    "        if self.result == 0:\n",
    "            return 0\n",
    "        elif self.result == -1:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                c = self.board[i, j]\n",
    "                c = 'X' if c == 1 else 'O' if c == 2 else '-'\n",
    "                print(c, end=' ')\n",
    "            print('')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(27, 243)\n",
    "#         self.fc2 = nn.Linear(81, 27)\n",
    "        self.fc3 = nn.Linear(243, 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "class Agent:\n",
    "    def __init__(self, alpha, gamma, epsilon, epsilon_min, epsilon_decay):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.dqn = DQN()\n",
    "        self.opt = torch.optim.Adam(self.dqn.parameters(), lr=self.alpha)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.memory = deque(maxlen = 100000)\n",
    "        \n",
    "    def choose_action(self, state, debug=False):\n",
    "        state_index = state.get_index()\n",
    "        legal_actions = state.legal_actions()\n",
    "        #print(legal_actions) #debug\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            action = legal_actions[np.random.randint(len(legal_actions))]\n",
    "        else:\n",
    "            actions = np.full(9, -5.0)\n",
    "            with torch.no_grad():\n",
    "                y = self.dqn(state_index).numpy()\n",
    "                if debug:\n",
    "                    print(y)\n",
    "                for i in legal_actions:\n",
    "                    actions[i] = y[0, i]\n",
    "            if debug:\n",
    "                print(actions)\n",
    "            action = np.argmax(actions)\n",
    "            #action = np.random.choice(np.flatnonzero(actions == actions.max()))\n",
    "        return action\n",
    "    \n",
    "    def remember(self, sarsA):\n",
    "        self.memory.append(sarsA)\n",
    "        \n",
    "    def replay(self):\n",
    "        y_batch, y_target_batch = [], []\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), 100))\n",
    "        state_index_batch = torch.cat([x[0] for x in minibatch])\n",
    "        y_batch = self.dqn(state_index_batch)\n",
    "#         y_target_batch = y_batch.clone().detach()\n",
    "        i = 0\n",
    "        for state_index, action, reward, next_state_index, legal_actions \\\n",
    "                in minibatch:\n",
    "#             print(state_index, action, reward, next_state_index, legal_actions)\n",
    "#             y = self.dqn(state_index)\n",
    "            y = y_batch[i]\n",
    "            y_target = y.clone().detach()\n",
    "            reward = torch.tensor(reward)\n",
    "            with torch.no_grad():\n",
    "                y_target[action] = reward if len(legal_actions) == 0 else \\\n",
    "                    reward + self.gamma * torch.max(self.dqn(next_state_index)[0, legal_actions])\n",
    "#             y_batch.append(y[0])\n",
    "            y_target_batch.append(y_target)\n",
    "            i += 1\n",
    "            \n",
    "#         y_batch = torch.cat(y_batch)\n",
    "        y_target_batch = torch.stack(y_target_batch)\n",
    "        \n",
    "        self.opt.zero_grad()\n",
    "        loss = self.criterion(y_batch, y_target_batch)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "            #self.update_Q(sarsA[0], sarsA[1], sarsA[2], sarsA[3], sarsA[4])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay\n",
    "        \n",
    "#         self.memory.clear()\n",
    "    \n",
    "    def update_Q(self, state_index, action, reward, next_state_index, legal_actions):\n",
    "        #print(legal_actions)\n",
    "        self.Q[state_index, action] = (1 - self.alpha) * self.Q[state_index, action] + \\\n",
    "                                         self.alpha * reward\n",
    "        if len(legal_actions) != 0:\n",
    "            #nxt = np.max(self.Q[next_state_index, legal_actions])\n",
    "            #if nxt != 0:\n",
    "               # print(\"nxt:\", )\n",
    "            self.Q[state_index, action] += self.alpha * self.gamma * \\\n",
    "                                            np.max(self.Q[next_state_index, legal_actions])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, ALPHA=0.01, GAMMA=1.0, EPSILON=1.0, EPSILON_DECAY=0.995, LAST_EPSILON=0):\n",
    "#         ALPHA = 0.003\n",
    "#         GAMMA = 1.0\n",
    "#         EPSILON = 0.327\n",
    "        EPSILON_MIN = 0.01\n",
    "        self.epsilon = EPSILON\n",
    "        self.last_epsilon = LAST_EPSILON\n",
    "#         EPSILON_DECAY = 1.0\n",
    "        self.p1 = Agent(ALPHA, GAMMA, EPSILON, EPSILON_MIN, EPSILON_DECAY)\n",
    "        self.p2 = Agent(ALPHA, GAMMA, EPSILON, EPSILON_MIN, EPSILON_DECAY)\n",
    "        self.state = State()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.p1.reset()\n",
    "        self.p2.reset()\n",
    "        self.state.reset()\n",
    "        \n",
    "    def learn(self, num_episodes):\n",
    "        self.p1.epsilon = self.p2.epsilon = self.epsilon\n",
    "        rs = np.zeros([10])\n",
    "        for i in range(num_episodes):\n",
    "            self.state.reset()\n",
    "            prev_p2_state = None\n",
    "            while True:\n",
    "                prev_p1_state = self.state.get_index()\n",
    "                p1_action = self.p1.choose_action(self.state)\n",
    "                now_p2_state = self.state.update(p1_action)\n",
    "                reward = self.state.get_reward()\n",
    "                if prev_p2_state is not None:\n",
    "                    legal_actions = self.state.legal_actions()\n",
    "                    self.p2.remember((prev_p2_state, p2_action, reward if reward != 1 else -1,\n",
    "                                      now_p2_state, legal_actions))\n",
    "                if reward != 0:\n",
    "                    self.p1.remember((prev_p1_state, p1_action, reward, None, []))\n",
    "                    break\n",
    "                    \n",
    "                prev_p2_state = now_p2_state\n",
    "                p2_action = self.p2.choose_action(self.state)\n",
    "                now_p1_state = self.state.update(p2_action)\n",
    "                reward = self.state.get_reward()\n",
    "                legal_actions = self.state.legal_actions()\n",
    "                self.p1.remember((prev_p1_state, p1_action, reward if reward != 1 else -1,\n",
    "                                  now_p1_state, legal_actions))\n",
    "                if reward != 0:\n",
    "                    self.p2.remember((prev_p2_state, p2_action, reward, None, []))\n",
    "                    break\n",
    "                \n",
    "#                 action = self.p1.choose_action(self.state)\n",
    "#                 state_index = self.state.get_index()\n",
    "#                 next_state_index = self.state.update(action)\n",
    "#                 reward = self.state.get_reward()\n",
    "#                 legal_actions = self.state.legal_actions()\n",
    "#                 self.p1.remember((state_index, action, reward, next_state_index, legal_actions))\n",
    "#                 if reward != 0:\n",
    "#                     break\n",
    "                \n",
    "#                 action = self.p2.choose_action(self.state)\n",
    "#                 state_index = self.state.get_index()\n",
    "#                 next_state_index = self.state.update(action)\n",
    "#                 reward = self.state.get_reward()\n",
    "#                 legal_actions = self.state.legal_actions()\n",
    "#                 self.p2.remember((state_index, action, reward, next_state_index, legal_actions))\n",
    "#                 if reward != 0:\n",
    "#                     break\n",
    "            \n",
    "            self.p1.replay()\n",
    "            self.p2.replay()\n",
    "            if num_episodes - i < 10:\n",
    "                self.p1.epsilon = self.p2.epsilon = self.last_epsilon\n",
    "                rs[num_episodes - i] = self.state.result\n",
    "            \n",
    "        first_won = sum((rs == 1).astype(int))\n",
    "        second_won = sum((rs == 2).astype(int))\n",
    "        draw = sum((rs == -1).astype(int))\n",
    "        #print(rs == 1)\n",
    "        return (first_won, second_won, draw)                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(ALPHA=0.01, EPSILON=0.3, GAMMA=0.7, EPSILON_DECAY=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d8a718927d4395b3e4e9418d502a62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 9)\n",
      "(0, 0, 9)\n",
      "(3, 2, 4)\n",
      "(0, 0, 9)\n",
      "(0, 0, 9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "for i in trange(5):\n",
    "#      env.p1.epsilon = 0.3\n",
    "#      env.p2.epsilon = 0.3\n",
    "    #print(env.p1.epsilon, env.p2.epsilon)\n",
    "    print(env.learn(800))\n",
    "# cnt = 0\n",
    "# for i in range(3**9):\n",
    "#     if np.sum(env.p1.Q[i, ]) > 2.7:\n",
    "#         #print(env.p1.Q[i, ])\n",
    "#         cnt  += 1\n",
    "# print(cnt)\n",
    "#for i in range(10):\n",
    "   # print(env.p1.Q[np.random.randint(3**9)])\n",
    "#print(np.sum(env.p1.Q[:, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.2339,  0.1556, -0.8679,  ..., -1.4896, -0.7616,  0.1076],\n",
      "        [-0.4072,  0.2342, -1.3025,  ..., -0.1416, -0.3626,  0.0964],\n",
      "        [-0.1299, -0.4486,  0.2227,  ..., -0.3393, -0.6022,  0.1188],\n",
      "        ...,\n",
      "        [-0.1619, -0.0657, -0.2739,  ..., -0.0895, -0.2674, -0.0437],\n",
      "        [-1.4632,  0.1178, -0.3631,  ..., -0.6100,  0.0053, -1.0096],\n",
      "        [-0.4208, -1.1076,  0.1053,  ..., -0.3191,  0.2866, -0.8766]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-6.3363e-01, -2.8839e-01, -3.3937e-01, -2.5867e-01, -2.0855e-01,\n",
      "        -4.0770e-01, -4.2733e-01, -7.4391e-01, -3.4704e-01, -3.2095e-01,\n",
      "        -6.3928e-01, -3.9254e-01, -8.2338e-01, -2.1011e-01, -8.3252e-01,\n",
      "        -2.3474e-01, -6.2762e-01, -6.1426e-02, -3.4527e-01, -4.8078e-01,\n",
      "        -5.9223e-01, -1.4345e-01, -6.3170e-01, -2.1476e-01, -6.1273e-01,\n",
      "        -2.0477e-01, -2.6297e-01, -2.6964e-01, -2.2097e-01, -1.9344e-01,\n",
      "        -2.7126e-01, -3.7851e-01, -2.4742e-01, -2.8758e-01, -3.9495e-01,\n",
      "        -1.6796e-01, -3.2653e-01, -4.6202e-01, -6.2548e-01, -3.6224e-01,\n",
      "        -1.2012e-01, -2.8742e-01, -4.6143e-01, -4.9617e-01, -5.9503e-01,\n",
      "        -2.5658e-01, -4.5470e-01, -3.6645e-03, -8.7683e-02, -9.4532e-01,\n",
      "        -2.8508e-01, -2.7384e-01, -1.0070e-01, -2.3160e-01, -1.9686e-01,\n",
      "        -1.5997e-01, -6.0199e-02, -1.2521e-01, -3.7824e-01, -4.0887e-01,\n",
      "        -1.7285e-01, -2.4066e-01, -6.6033e-01, -3.9875e-01, -5.9726e-02,\n",
      "        -6.3829e-01, -4.0102e-01, -5.1159e-01, -2.9559e-01, -4.3350e-01,\n",
      "        -1.2631e-01, -1.8464e-01, -5.2648e-01,  8.7584e-03, -4.5398e-01,\n",
      "        -3.7922e-01, -5.4330e-01, -1.8463e-01, -3.0737e-01, -5.1251e-01,\n",
      "        -1.1918e-01, -3.5804e-01,  3.2739e-02, -4.1419e-01, -7.3421e-01,\n",
      "        -5.4150e-01, -2.0942e-01, -5.3164e-01, -3.9421e-01, -2.1513e-01,\n",
      "        -7.3395e-02, -1.7901e-01, -1.4902e-01, -5.4278e-01, -2.1069e-01,\n",
      "        -1.8090e-01, -3.1480e-01, -5.2367e-01, -1.9096e-01, -6.4764e-01,\n",
      "        -2.2895e-03, -4.3517e-04,  8.1689e-02, -2.7784e-01, -1.0448e-01,\n",
      "        -5.4150e-01, -1.2590e-01, -5.3443e-01, -2.5413e-01, -1.5608e-01,\n",
      "        -2.6342e-01, -6.2242e-01, -4.4041e-01, -4.6402e-01, -2.0019e-01,\n",
      "        -4.9360e-01, -3.3793e-01, -4.2219e-01, -1.4498e-01, -7.1948e-01,\n",
      "        -3.5720e-01,  2.4631e-02, -6.3033e-01, -5.5025e-01, -2.0507e-01,\n",
      "        -5.4920e-01, -5.6669e-01, -3.7531e-01, -2.7823e-01, -4.6733e-01,\n",
      "        -1.7975e-01, -3.3627e-01, -1.1924e-01, -2.3184e-01, -2.3215e-01,\n",
      "        -4.4121e-01, -2.0309e-01, -3.8871e-01, -4.2442e-01, -5.2828e-01,\n",
      "        -3.8977e-01, -5.7645e-01, -8.8719e-02, -5.5508e-01, -3.8255e-01,\n",
      "        -3.0936e-01, -6.5831e-01, -4.1930e-02, -6.0231e-02, -6.9551e-01,\n",
      "        -6.1919e-01, -5.3292e-01, -1.9959e-01, -5.0886e-01, -5.3385e-01,\n",
      "        -3.6488e-01, -5.1748e-01,  3.9633e-02, -2.4353e-01, -6.2919e-01,\n",
      "        -1.7877e-01, -4.1511e-01, -4.1428e-01, -1.0481e-01, -4.6930e-01,\n",
      "        -5.0444e-01, -4.9047e-01, -5.2321e-01, -5.5727e-01, -2.8399e-01,\n",
      "        -6.4829e-02, -2.6187e-01, -6.7871e-01, -1.9271e-01, -6.1640e-01,\n",
      "        -7.6816e-01, -7.4150e-02, -5.5756e-01, -2.8335e-01, -8.6313e-02,\n",
      "        -1.2130e-01, -4.8029e-01, -7.1850e-01, -2.5631e-01, -4.1759e-01,\n",
      "        -2.1016e-01, -2.2965e-01, -3.1395e-01, -7.4058e-01, -4.5519e-01,\n",
      "        -7.2058e-02, -3.2817e-01, -6.5076e-01, -1.8734e-01, -4.5622e-01,\n",
      "        -4.5904e-01, -2.2548e-01, -6.3301e-01, -6.4897e-01, -3.7382e-02,\n",
      "        -1.4190e-01, -4.5040e-01, -1.3162e-02, -6.5348e-01, -5.0473e-01,\n",
      "        -3.7577e-01,  2.1541e-02, -3.4054e-01, -7.1196e-02, -3.9215e-01,\n",
      "        -1.8516e-01, -4.4249e-01, -3.4134e-01, -1.0552e-01, -2.5351e-01,\n",
      "        -2.1105e-01, -1.3597e-01, -1.9323e-01, -3.9471e-01, -1.1019e-01,\n",
      "        -1.9648e-01, -4.8000e-01, -2.0210e-01, -7.6305e-02, -5.3595e-01,\n",
      "         8.4765e-02, -2.7379e-01, -3.6256e-01, -3.2422e-01, -3.3869e-01,\n",
      "        -2.9268e-01, -3.1935e-01, -3.0481e-01, -4.7515e-01, -4.8405e-01,\n",
      "        -3.6245e-01, -1.8491e-01, -6.3525e-01, -7.6828e-01, -1.5734e-01,\n",
      "        -1.1572e-01, -6.1256e-01, -9.4348e-01], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2203,  0.3680,  0.1428,  ..., -0.0407,  0.3411,  0.1800],\n",
      "        [ 0.0555,  0.1006, -0.3546,  ..., -0.0159, -0.6524, -0.0637],\n",
      "        [ 0.2409,  0.2752,  0.1565,  ...,  0.0212,  0.2029, -0.1682],\n",
      "        ...,\n",
      "        [ 0.2934,  0.3449,  0.1473,  ...,  0.0169, -0.2793,  0.3436],\n",
      "        [ 0.0474,  0.0713, -0.1128,  ..., -0.0398, -0.6471,  0.0697],\n",
      "        [ 0.0244,  0.0570,  0.0079,  ..., -0.0092, -0.0867,  0.1430]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.3665, 0.2658, 0.3118, 0.2788, 0.3507, 0.3310, 0.3341, 0.2810, 0.3491],\n",
      "       requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.2018, -0.0726, -0.3918,  ..., -0.0525, -0.2345, -0.0675],\n",
      "        [ 0.0861, -2.3521,  0.7228,  ...,  0.5907, -0.7280, -1.2255],\n",
      "        [-0.9888,  0.5972, -2.6149,  ..., -0.0800, -0.9638,  0.8229],\n",
      "        ...,\n",
      "        [ 0.3879, -1.2208, -1.0967,  ..., -1.0314,  0.2060,  0.3390],\n",
      "        [ 0.1046, -0.7081, -0.4964,  ..., -1.8957,  0.5747, -0.8191],\n",
      "        [-0.0096, -0.0795, -0.0536,  ..., -0.1355, -0.0336, -0.1764]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0046, -0.4318, -0.2020,  0.0016, -0.2948, -0.1065, -0.1560, -0.4031,\n",
      "        -0.3420, -0.3473, -0.5597, -0.3589, -0.6899, -0.2771, -0.5547, -0.4434,\n",
      "        -0.6356, -0.0595, -0.5704,  0.0563, -0.1036, -0.1130, -0.5904, -0.3044,\n",
      "        -0.2655, -0.1012, -0.2836, -0.0740, -0.2304, -0.0369, -0.1600, -0.5576,\n",
      "        -0.5247, -0.1457, -0.3409, -0.4492, -0.4463, -0.3091, -0.4073, -0.3832,\n",
      "        -0.4890, -0.4487, -0.5298, -0.4860, -0.3552, -0.2009, -0.2433, -0.2944,\n",
      "        -0.5761, -0.0280, -0.5961, -0.4682, -0.2261, -0.2852, -0.3492, -0.3212,\n",
      "        -0.4325, -0.1786, -0.5316, -0.3384, -0.3792, -0.0131, -0.2963, -0.2545,\n",
      "        -0.5574, -0.1376, -0.3788, -0.1290, -0.1136, -0.1661, -0.7364, -0.1706,\n",
      "        -0.2716, -0.3707, -0.5828, -0.7111, -0.2895, -0.5895, -0.4154, -0.4909,\n",
      "        -0.3343, -0.1517, -0.3109, -0.4714, -0.2170, -0.5664, -0.6225, -0.0318,\n",
      "        -0.0861, -0.7223,  0.0762, -0.2241, -0.5149, -0.0890, -0.2627, -0.6329,\n",
      "        -0.5236, -0.1253, -0.3142, -0.4674, -0.2329, -0.5931, -0.1269, -0.3508,\n",
      "        -0.1343, -0.6688, -0.2483, -0.2970, -0.0329, -0.4684, -0.2435, -0.1745,\n",
      "        -0.0428, -0.4323, -0.4787, -0.2566, -0.2663, -0.3783, -0.4540, -0.5228,\n",
      "        -0.4055, -0.4348, -0.7105, -0.4273, -0.2701, -0.5352, -0.5729, -0.6239,\n",
      "        -0.2473, -0.1643, -0.2991, -0.6561, -0.3994, -0.4195, -0.2331, -0.5006,\n",
      "        -0.1728, -0.8764, -0.2404,  0.0047, -0.2303, -0.1833, -0.1775,  0.0302,\n",
      "        -0.1481, -0.5363, -0.3299, -0.3408, -0.5545, -0.2187, -0.6291, -0.1620,\n",
      "        -0.3054, -0.0500, -0.2450, -0.0722, -0.4028, -0.7136, -0.4411,  0.0047,\n",
      "        -0.4663, -0.5120, -0.4558, -0.2538, -0.2579, -0.4758, -0.1597, -0.4998,\n",
      "        -0.4503, -0.2657, -0.1828, -0.2387, -0.0570, -0.2174, -0.4406, -0.6076,\n",
      "        -0.5161, -0.0866, -0.4042, -0.0939, -0.3986, -0.1679, -0.3321, -0.1907,\n",
      "        -0.3256, -0.7026, -0.0362, -0.0330, -0.0239, -0.3019, -0.2272, -0.2057,\n",
      "        -0.5693, -0.3506, -0.5038, -0.1311, -0.5341, -0.2935, -0.0057, -0.5244,\n",
      "        -0.4935, -0.5522, -0.2147, -0.2098, -0.3876, -0.4601, -0.0037,  0.0776,\n",
      "        -0.4343, -0.4927, -0.5081, -0.2205, -0.2813, -0.0850, -0.0223, -0.2976,\n",
      "        -0.4923, -0.2087, -0.4119, -0.4785, -0.3662, -0.3860, -0.0057, -0.2573,\n",
      "        -0.1297,  0.0483, -0.4883, -0.6338, -0.3025, -0.2479, -0.3676,  0.1374,\n",
      "        -0.3647, -0.3861, -0.2315, -0.1875, -0.3027, -0.7083, -0.1505, -0.4589,\n",
      "        -0.4497, -0.2620, -0.0493], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0217, -0.1987,  0.1317,  ...,  0.1920,  0.2054,  0.0327],\n",
      "        [-0.0547,  0.1282,  0.0759,  ..., -0.5731, -0.0496, -0.0720],\n",
      "        [-0.0213, -0.3753,  0.1345,  ..., -0.0116, -0.0338, -0.0634],\n",
      "        ...,\n",
      "        [-0.0772, -0.8351, -0.4422,  ..., -0.0267, -0.1089, -0.0140],\n",
      "        [-0.0338,  0.4503, -0.4910,  ..., -0.0909, -0.3715, -0.0252],\n",
      "        [-0.0821, -0.5555, -0.5935,  ...,  0.0700,  0.1824,  0.0079]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.2343, 0.1549, 0.2493, 0.1863, 0.3095, 0.0343, 0.1920, 0.0404, 0.1980],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "p1 = copy.deepcopy(env.p1)\n",
    "p1.epsilon = 0\n",
    "print([x for x in p1.dqn.parameters()])\n",
    "p2 = copy.deepcopy(env.p2)\n",
    "p2.epsilon = 0\n",
    "print([x for x in p2.dqn.parameters()])\n",
    "game = State()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - \n",
      "- - - \n",
      "- - - \n",
      "4\n",
      "- - - \n",
      "- X - \n",
      "- - - \n",
      "[[ 0.2162276   0.08804827  0.19759193  0.1177718   0.34314924 -0.01184662\n",
      "   0.13096912 -0.01980584  0.18402025]]\n",
      "[ 0.21622761  0.08804827  0.19759193  0.1177718  -5.         -0.01184662\n",
      "  0.13096912 -0.01980584  0.18402025]\n",
      "\n",
      "O - - \n",
      "- X - \n",
      "- - - \n",
      "1\n",
      "O X - \n",
      "- X - \n",
      "- - - \n",
      "[[-0.40721548  0.28417298 -1.3313626  -1.3842998   1.5233128  -0.8686765\n",
      "  -0.9423363   0.22546801 -1.5427141 ]]\n",
      "[-5.         -5.         -1.33136261 -1.38429976 -5.         -0.86867648\n",
      " -0.94233632  0.22546801 -1.54271412]\n",
      "\n",
      "O X - \n",
      "- X - \n",
      "- O - \n",
      "3\n",
      "O X - \n",
      "X X - \n",
      "- O - \n",
      "[[ 1.7340415   0.4855237  -0.77528673  0.12298936 -0.05742446  0.36794394\n",
      "  -0.6440845   0.33592376 -0.8002629 ]]\n",
      "[-5.         -5.         -0.77528673 -5.         -5.          0.36794394\n",
      " -0.64408451 -5.         -0.80026293]\n",
      "\n",
      "O X - \n",
      "X X O \n",
      "- O - \n",
      "8\n",
      "O X - \n",
      "X X O \n",
      "- O X \n",
      "[[ 1.5829349   0.32340115  0.55825675  0.69160366 -0.87538636  0.4214168\n",
      "  -0.22652675  0.6398905  -1.7852392 ]]\n",
      "[-5.         -5.          0.55825675 -5.         -5.         -5.\n",
      " -0.22652675 -5.         -5.        ]\n",
      "\n",
      "O X O \n",
      "X X O \n",
      "- O X \n",
      "6\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "game.reset()\n",
    "while True:\n",
    "#     #cpu first\n",
    "#     game.print_board()\n",
    "#     action = p1.choose_action(game, debug=True)\n",
    "#     #state_index = game.get_index()\n",
    "#     next_state_index = game.update(action)\n",
    "#     reward = game.get_reward()\n",
    "#     if reward != 0:\n",
    "#         break\n",
    "#     print()\n",
    "#     game.print_board()\n",
    "#     action = int(input())\n",
    "#     #state_index = self.state.get_index()\n",
    "#     next_state_index = game.update(action)\n",
    "#     reward = game.get_reward()\n",
    "#     #legal_actions = self.state.legal_actions()\n",
    "#     if reward!= 0:\n",
    "#         break\n",
    "        \n",
    "    # player first\n",
    "    game.print_board()\n",
    "    action = int(input())\n",
    "    #state_index = self.state.get_index()\n",
    "    next_state_index = game.update(action)\n",
    "    reward = game.get_reward()\n",
    "    #legal_actions = self.state.legal_actions()\n",
    "    if reward != 0:\n",
    "        break\n",
    "    game.print_board()\n",
    "    action = p2.choose_action(game, debug=True)\n",
    "    #state_index = game.get_index()\n",
    "    next_state_index = game.update(action)\n",
    "    reward = game.get_reward()\n",
    "    if reward != 0:\n",
    "        break\n",
    "    print()\n",
    "            \n",
    "print(game.result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "dummy = np.zeros(200)\n",
    "print(sum((dummy == 0).astype(int)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
